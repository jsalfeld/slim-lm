# SFT Full Fine-tuning (no LoRA)
method: sft
model_name: meta-llama/Llama-3.1-8B
output_dir: outputs/sft_full

# Data
train_data: data/sft_train.jsonl
eval_data: data/sft_eval.jsonl

# No LoRA - full fine-tuning
use_lora: false
use_qlora: false

# Training
num_epochs: 3
batch_size: 2
gradient_accumulation_steps: 8
learning_rate: 2.0e-5
max_seq_length: 2048
warmup_ratio: 0.03

# Logging
logging_steps: 10
save_steps: 100
eval_steps: 100
