# Nash-MD Configuration (Nash Learning from Human Feedback)
# Game-theoretic approach finding Nash equilibrium
method: nash
model_name: meta-llama/Llama-3.1-8B
output_dir: outputs/nash

# Data (needs prompts)
train_data: data/train.jsonl
eval_data: data/eval.jsonl

# LoRA
use_lora: true
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05

# Quantization
use_qlora: true

# Nash-MD specific
beta: [0.1]  # KL penalty coefficients
mixture_coef: [0.5, 0.5]  # Policy mixture coefficients
loss_type: sigmoid

# Generation
max_new_tokens: 256
max_length: 512
temperature: 0.9

# Training
num_epochs: 1
batch_size: 2
gradient_accumulation_steps: 8
learning_rate: 5.0e-7
warmup_ratio: 0.1

# Logging
logging_steps: 10
save_steps: 100
