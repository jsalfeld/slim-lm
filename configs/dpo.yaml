# DPO Configuration
method: dpo
model_name: outputs/sft  # Start from SFT model
output_dir: outputs/dpo

# Data (needs prompt, chosen, rejected columns)
train_data: data/train.jsonl
eval_data: data/eval.jsonl

# LoRA
use_lora: true
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05

# Quantization
use_qlora: true

# DPO specific
beta: 0.1
loss_type: sigmoid  # sigmoid, hinge, ipo

# Training
num_epochs: 1
batch_size: 2
gradient_accumulation_steps: 8
learning_rate: 5.0e-5
max_seq_length: 2048
max_prompt_length: 1024
warmup_ratio: 0.1

# Logging
logging_steps: 10
save_steps: 100
eval_steps: 100
