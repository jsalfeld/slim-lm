# SFT Configuration
method: sft
model_name: meta-llama/Llama-3.1-8B
output_dir: outputs/sft

# Data
train_data: data/train.jsonl
eval_data: data/eval.jsonl

# LoRA (set use_lora: false for full fine-tuning)
use_lora: true
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05

# Quantization (set use_qlora: false for no quantization)
use_qlora: true

# Training
num_epochs: 3
batch_size: 4
gradient_accumulation_steps: 4
learning_rate: 2.0e-4
max_seq_length: 2048
warmup_ratio: 0.03

# Logging
logging_steps: 10
save_steps: 100
eval_steps: 100
