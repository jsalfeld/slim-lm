# DPO Full Fine-tuning (no LoRA)
method: dpo
model_name: outputs/sft_full
output_dir: outputs/dpo_full

# Data
train_data: data/train.jsonl
eval_data: data/eval.jsonl

# No LoRA
use_lora: false
use_qlora: false

# DPO specific
beta: 0.1
loss_type: sigmoid

# Training
num_epochs: 1
batch_size: 1
gradient_accumulation_steps: 16
learning_rate: 5.0e-6
max_seq_length: 2048
max_prompt_length: 1024
warmup_ratio: 0.1

# Logging
logging_steps: 10
save_steps: 100
eval_steps: 100
